#!/usr/bin/env python
'''
Routines for obtaining, generating, and processing meta data about data files, 
file runs, and functions used in the data file generation process. 

The way that the metadata scheme works is:

In the top level System Folder there is a directory ../System/MetaData that 
contains a replica of the file structure outside of the System folder.   
Each "real" path corresponds to a directory in the MetaData directory,
in which metadata about the path is stored, 
e.g. '../Data/Dan_Data/NPR_Puzzle_Solutions', would correspond to 
'../System/MetaData/Data/Dan_Data/NPR_Puzzle_Solutions'.   
Metadata is similarly attached to functions in python modules referenced via their dot-paths.   
    
Given a file path P, the associated path for the metadata is given by the 
function metadatapath(P).  Given a python function dot path, the associated 
path P for the metadata is given by the function opmetadatapath(P).   

Inside a given path's metadata directory is a variety of associated metadata files. 
There are three main kinds of associated things:
    -- data generated during the runtime that produced/was generated by the path. 
    -- metadata attached by the human directly to the file.  
    -- data used and generated by the system Graphical Browser.  
    e.g. Linklists local to the path, and graphviz-generated graphs of these LinkLists.  
    (See commends in System/SystemGraphOperations.py for information about this) 
    
'''



import marshal
import os
import types
import hashlib
import traceback
import tabular as tb
import cPickle as pickle

from starflow.utils import *
from starflow.storage import StoredDocstring
from starflow import static
import starflow.de as de
DE_MANAGER = de.DataEnvironmentManager()
WORKING_DE = DE_MANAGER.working_de

def AttachMetaData(NewMetaData,FileName = '',OperationName='',Resources = None,
                   creates = WORKING_DE.relative_metadata_dir):

    '''
    Attach metadata to a file a given path.    
    
    ARGUMENTS:
    --NewMetaData : a dictionary that will be pickled as metadata.  
    --FileName = Path of the file to which the metadata should be attached.
    --OperationName = Python dot-path of the operati which the metadata should 
    be attached. 
    (Either File or OperationName should be specified, but not both.)
        
    Metadata attached through this process is put into the file 
        MetaDataPath/AttachedMetaData.pickle.
    If this path exists at the time that an AttachMetaData command is run,
    the metadata dictionary in the file is updated with the NewMetaData dictionary. 
        
    '''

    assert isinstance(NewMetaData,dict), 'The metadata to attach must be a dictionary.'
    
    if FileName != '':
        metapath = metadatapath(FileName)
    elif OperationName != '':
        metapath = opmetadatapath(OperationName)
    
    if not PathExists(metapath):
        MakeDirs(metapath)
    
    mdp = metapath + '/AttachedMetaData.pickle'
    if PathExists(mdp):
        try:
            ExistingMetaData = pickle.load(open(mdp,'rb'))
            if not isinstance(ExistingMetaData,dict):
                ExistingMetaData = {}
        except:
            ExistingMetaData = {}
    else:
        ExistingMetaData = {}
    
    ExistingMetaData.update(NewMetaData)
    
    F = open(mdp,'w')
    pickle.dump(ExistingMetaData,F)
    F.close()
    
    ProcessMetaData(metapath,FileName if FileName else OperationName,extensions=['Attached'])


def MakeRuntimeMetaData(opname,Creates,OriginalTimes,OriginalDirInfo,RunOutput,ExitType,ExitStatus,Before,After,IsDifferent,TempSOIS):

    '''
    This is an internal usage function that attaches the results of 
    metadata generated during the running of a system update.  
    It is used primilary by the UpdateLinks function in ../System/Update.py
    
    Suppose the function F is run by the automatic updater.   
    During this runtime, several kinds of data are produced:  
    --the return output of the function F, and 
    --information about the run, its Exit status, whether it changed any 
        files it was meant to produce, etc...
    
    These pieces of runtime metadata are collected by the UpdateLink fnction 
    during update, and then passed to MakeRuntimeMetaData function to write 
    out the data in an appropriate format to the metadata files. 
    
    in summary, it
    -- writes output of the executed function F to a file at path 
        opmetadatapath(F) + '/RuntimeOutput.pickle'
    -- appends exit status information to opmetadatapath(F) + '/ExitStatusRecord.csv'
    -- for each file j created by the running of F:
        -- appends file-specific creation information to 
        metadatapath(j) + '/CreationRecord.csv'
        -- attaches MetaData returned by the script for file j to
        metadatapath(j) + '/AssociatedMetaData.pickle'

    '''
    
    MakeAutomaticMetaData(opname,usedefault=True)
    
    for j in Creates:
        if not PathExists(metadatapath(j)):
            MakeDirs(metadatapath(j))
    if not PathExists(opmetadatapath(opname)):
        MakeDirs(opmetadatapath(opname))
    
    rtime = str(After - Before)
    
    strongcopy(opmetadatapath(opname) + '/MostRecentRunPrintout.txt',opmetadatapath(opname) + '/PreviousRunPrintout.txt')
    delete(opmetadatapath(opname) + '/MostRecentRunPrintout.txt')
    strongcopy(TempSOIS,opmetadatapath(opname) + '/MostRecentRunPrintout.txt')

    for j in Creates:
        CRFName = metadatapath(j) + '/CreationRecord.csv'
        if not PathExists(CRFName):
            F = open(CRFName,'w')
            F.write('FileName,Operation,ExitType,TimeStamp,OriginalTimeStamp,Runtime,Diff\n')
            F.close()                               
            
        F = open(CRFName,'a')
        F.write(','.join([j,opname,ExitType,str(os.path.getmtime(j) if PathExists(j) else nan),str(OriginalTimes[j]),rtime,str(int(IsDifferent[j]))]) + '\n')
        F.close()   
    
    for j in OriginalDirInfo.keys():
        CRFName = metadatapath(j) + '/CreationRecord.csv'
        Ptime = FindPtime(j)        
        if PathExists(CRFName):
            F = open(CRFName,'a')
            F.write(','.join([j,opname,'FromBelow',str(os.path.getmtime(j) if PathExists(j) else nan),str(OriginalDirInfo[j][0]),'0.0','1']) + '\n')
            F.close()   
    
    F = open(opmetadatapath(opname) + '/RuntimeOutput.pickle','w')
    pickle.dump(RunOutput,F)
    F.close()
    
    ESFName = opmetadatapath(opname) + '/ExitStatusFile.csv'
    if not PathExists(ESFName):
        F = open(ESFName,'w')
        F.write('OperationName,ExitType,ExitStatus,TimeStamp,CreateList,CreateTimeStamps,Before,After,Runtime\n')
        F.close()           
    CreateString = '\t'.join(Creates)
    CreateTimesList = [os.path.getmtime(l) if PathExists(l) else numpy.nan for l in Creates]
    CreateTimesString = '\t'.join([str(x) for x in CreateTimesList])
    TS = str(max(Before,max(CreateTimesList)))
    NewESFData = ','.join([opname,ExitType, str(ExitStatus),TS , CreateString , CreateTimesString,str(Before),str(After),rtime]) + '\n'
    esf = open(ESFName,'a') 
    esf.write(NewESFData)
    esf.close()     
                
    if ExitType == 'Success':
        Written = []
        if isinstance(RunOutput,dict) and 'MetaData' in RunOutput.keys() and isinstance(RunOutput['MetaData'],dict):    
            for j in RunOutput['MetaData'].keys():
                if isinstance(j,str):
                    if any([PathAlong(j,k) for k in Creates]) or WORKING_DE.protection != 'ON':
                        if not PathExists(metadatapath(j)):
                            MakeDirs(metadatapath(j))
                        mdp = metadatapath(j) + '/AssociatedMetaData.pickle'
                        
                        if PathExists(mdp):
                            oldmetadata = pickle.load(open(mdp,'rb'))
                        else:
                            oldmetadata = None                  
                        newmetadata = RunOutput['MetaData'][j]
                        if newmetadata != oldmetadata:
                            metadatafile = open(mdp,'w')
                            pickle.dump(newmetadata,metadatafile)
                            metadatafile.close()
                        print '\nMetaData for ', j, 'written ...'
                        Written.append(mdp)         
                        ProcessMetaData(metadatapath(j),objname=j,extensions=['Associated'])

                    else:
                        print 'Attempt to write runtime metadata for', j,  ' blocked because Protection is ON and ', j , ' is not contained in any of the declared creates: ', Creates, '.'
                else:
                    print 'Attempt to write runtime metadata for', j, ' failed because', j ,'does not appear to be a path string.'
        else:
            print 'No runtime metadata output for ', opname , ', or metadata argument in wrong format.'
    
        BadSet = ListUnion([[kk for kk in RecursiveFileList(metadatapath(k)) if (kk.endswith('AssociatedMetaData.pickle')) and kk not in Written] for k in Creates])
        for kk in BadSet:
            'Deleting', kk, 'as it appears to old irrelevant associated metadata.'
            delete(kk)

    if ExitType == 'Success':
        for j in Creates:
            if IsDifferent[j] and PathExists(j):
                MakeAutomaticMetaData(j)

def MakeAutomaticMetaData(objname,usedefault=False,forced=False,
                                   creates = WORKING_DE.relative_metadata_dir,**kwargs):
    print 'Generating automatic metadata for ', objname, '...'
    try:
        SF = __import__(static.LOCAL_SETUP_MODULE)
        reload(SF)
    except:
        traceback.print_exc()
        print 'Failed to import SetupFunctions module.  Using default automatic metadata module.'
        X = DEFAULT_GenerateAutomaticMetaData(objname)
    else:
        try:
            metadatafn = getattr(SF,static.LOCAL_METADATA_GENERATOR)
            X = metadatafn(objname,forced=forced,**kwargs)
        except:
            print 'Automatic generation of metadata for', objname, 'by user-defined function failed (using default instead).  Here is the error:'
            traceback.print_exc()
            X = DEFAULT_GenerateAutomaticMetaData(objname)
    
    metapath = opmetadatapath(objname) if IsDotPath(objname) else metadatapath(objname)
    
    if isinstance(X,dict):
        for j in X.keys():
            metapathj = opmetadatapath(j) if IsDotPath(j) else metadatapath(j)
            if PathAlong(metapathj,metapath) or (IsDotPath(j) and metapath == metapathj):
                if not PathExists(metapathj):
                    MakeDirs(metapathj)
                F = open(metapathj + '/AutomaticMetaData.pickle','w')
                pickle.dump(X[j],F)
                F.close()
    
                ProcessMetaData(metapathj,j,extensions=['Automatic'])
        print '... done generating automatic metadata for ', objname, '.'
    else:
        print 'Automatically generated metadata for', objname, 'is not in proper format.'

    

def DEFAULT_GenerateAutomaticMetaData(objname):
    
    D = {}
    
    if IsPythonFile(objname) or IsDotPath(objname):
        D['Verbose'] = StoredDocstring(objname)

    if D:
        return  {objname : D}
    else:
        return {}


def ProcessMetaData(metapath,objname=None, extensions = None, usedefault=False,
                                         depends_on = WORKING_DE.relative_metadata_dir):
    if objname is None:
        objname = metapath

    try:
        SF = __import__(static.LOCAL_SETUP_MODULE)
        reload(SF)
    except :
        print 'Failed to import SetupFunction module.  Using Default MetaData Processor.'
        DEFAULT_MetaDataProcessor(metapath,objname=objname,extensions=extensions)
    else:
        try:
            metadata_processor = getattr(SF,static.LOCAL_METADATA_PROCESSOR)
            metadata_processor(metapath,objname=objname,extensions=extensions)
        except:
            print 'Processing of metadata for', objname, 'by user-defined Processor function failed  (using default instead).  Here is the error:'
            traceback.print_exc()
            DEFAULT_MetaDataProcessor(metapath,objname=objname,extensions=extensions)


def DEFAULT_MetaDataProcessor(metapath,objname = None, extensions = None):

    if objname is None:
        objname = metapath
    
    X = ConsolidateSources(metapath)
    ProcessResources(metapath,X,objname)
    image = ChooseImage(metapath)
    if image:
        X['image'] = image
    F = open(metapath + '/ProcessedMetaData.pickle','w')
    pickle.dump(X,F)
    
    text = SummarizeMetaData(X)
    F = open(metapath + '/MetaDataSummary.html','w')
    F.write(text)
    F.close()


def ConsolidateSources(metapath,objname=None,extensions = None):

    consolidated = CombineSources(metapath,keys = ['Resources','author','keywords','signature','title','description','Verbose'],extensions=extensions)
    
    if 'Resources' in consolidated:
        consolidated['Resources'] = uniqify(ListUnion(consolidated['Resources'].values()))
    
    if 'author' in consolidated.keys():
        consolidated['author'] = '; '.join(consolidated['author'].values())
    
    if 'title' in consolidated.keys():
        consolidated['title'] = '; '.join(consolidated['title'].values())
        
    if 'description' in consolidated.keys():
        descrs = consolidated['description'].items()
        if len(descrs) == 1:
            consolidated['description'] = descrs[0][1]
        else:
            consolidated['description'] = '\n\n'.join([e + ': ' + d for (e,d) in descrs])
            
    elif 'Verbose' in consolidated.keys():
        descrs = consolidated['Verbose'].items()
        if len(descrs) == 1:
            consolidated['description'] = descrs[0][1]
        else:
            consolidated['description'] = '\n\n'.join([e + ': ' + d for (e,d) in descrs])
    
    if 'keywords' in consolidated.keys():
        for k in consolidated['keywords'].keys():
            if not is_string_like(consolidated['keywords'][k]):
                consolidated['keywords'][k] = ','.join(consolidated['keywords'][k])
                
        consolidated['keywords'] = [x.strip() for x in uniqify((','.join(consolidated['keywords'].values())).split(','))]
                
    if 'signature' in consolidated.keys():
        s = uniqify(consolidated['signature'].values())
        if len(s) == 1:
            consolidated['signature'] = s[0]
        else:
            consolidated['signature'] = ''

    return consolidated
    
    
    
def SummarizeMetaData(X):

    if 'image' in X.keys():
        image = '<img src="' + X['image'] + '"/><br/>'
    else:
        image = ''

    if 'description' in X.keys():
        description = '<strong>Description: </strong>' + X['description'].replace('\n','<br/>')
    else:
        description = ''
    
    if 'author' in X.keys():
        author = '<strong>Author: </strong>' + X['author']
    else:
        author = ''
    
    if 'title' in X.keys():
        title = '<strong>Title: </strong>' + X['title']
    else:
        title = ''
    
    if 'keywords' in X.keys():
        keywords = '<strong>Keywords: </strong>' + ','.join(X['keywords'])
    else:
        keywords = ''
    
    if 'signature' in X.keys():
        signature = '<strong>Signature: </strong> This appears to be a ' + X['signature'] + ' file.'  
    else:
        signature = ''

    
    text = '<br/>'.join([x for x in [image,title,author,signature,description,keywords] if x != ''])
    
    return text
    
    
def ChooseImage(metapath):
    images = [metapath[2:] + '/' + l for l in listdir(metapath) if 'image' in l.lower() and l.lower().endswith(('.png','.pdf','.gif','.jpg','.tiff','.bmp'))]
    if len(images) > 0:
        return images[0]


def CombineSources(metapath,keys=None,extensions=None,depends_on = WORKING_DE.relative_metadata_dir):

    if extensions is None:
        extensions = ['Attached','Associated','Automatic']

    consolidated = {} 
    for ext in extensions:
        fileext = '/' + ext + 'MetaData.pickle'
        if PathExists(metapath + fileext):
            metadata = pickle.load(open(metapath + fileext,'r'))
            if is_string_like(metadata):
                metadata = {'description' : metadata}

            for k in metadata.keys():
                if keys is None or k in keys:
                    if k not in consolidated:
                        consolidated[k] = {ext : metadata[k] }
                    else:
                        consolidated[k].update({ext : metadata[k]})
    
    return consolidated


def ProcessResources(metapath,metadata,objname,depends_on=WORKING_DE.relative_root_dir):
    if isinstance(metadata,dict) and 'Resources' in metadata.keys():
        Resources = metadata['Resources']
        if Resources:
            for (source,name) in Resources:
                if is_file_name(name):
                    mdp = os.path.join(metapath,name)
                    if is_external_url(source):
                        E = os.system('wget ' + source + ' -O ' + mdp)
                        if E != 0 or not PathExists(mdp):
                            print 'Error attaching metadata to', objname, ': appears to have been unable to locate download URL', source
                    else:
                        if is_string_like(source) and PathExists(source):
                            try:
                                strongcopy(source,mdp)
                            except:
                                print 'Error processing resource metadata to', objname, ': source name',source, 'does not describe a path on the system or a URL.'
                        else:
                            print 'Error processing resource metadata to', objname, ': source name',source, 'does not describe a path on the system or a URL.'
                else:
                    print 'Error processing resource metadata to', objname, ': metadata name', name, 'isn\'t a file name'
            
            

def tabularmetadataforms(pathlist,depends_on = WORKING_DE.relative_metadata_dir):
    attlist = ['description','author','title','keywords']
    recs1 = []
    recs2 = []
    for x in pathlist:
        print x
        mdp = metadatapath(x) + '/ProcessedMetaData.pickle'
        if PathExists(mdp):
            M = pickle.load(open(mdp))
            D = {}
            for att in attlist:
                if att in M.keys():
                    D[att] = M[att]
                else:
                    D[att] = ''
            recs1.append((x,) + tuple([D[att].replace('\n',' ') for att in attlist]))
            colnames = M['colnames']
            if 'coldescrs' in M.keys():
                coldescrs = [M['coldescrs'][m] if m in M['coldescrs'].keys() else ''  for m in colnames]
            else:
                coldescrs = ['']*len(colnames)
            
            recs2 += zip([x]*len(colnames),colnames,coldescrs)      
        
    X = tb.tabarray(records = recs1,names=['Path'] + attlist)
    Y = tb.tabarray(records = recs2,names = ['Path','ColName','ColDescr'])
    
    return [X,Y]
    
def copymetadata(path,to,depends_on = WORKING_DE.relative_metadata_dir):
    strongcopy(os.path.join(metadatapath(path), 'ProcessedMetaData.pickle'),to)
    
def loadmetadata(path,depends_on = WORKING_DE.relative_metadata_dir):
    processed_file = os.path.join(metadatapath(path) , '/ProcessedMetaData.pickle')
    if PathExists(process_file):
        return pickle.load(open(processed_file,'rb'))

def IsFailure(Path):
    '''
    Returns Boolean True if Path represents is the python dot-path 
    of an operation whose most recent run by the autmatic updater 
    was a failure.
    '''
    metapath = opmetadatapath(Path) + '/ExitStatusFile.csv'
    if PathExists(metapath):
        try:
            ESD = tb.tabarray(SVfile = metapath,delimiter = ',', lineterminator='\n') 
            ESD.sort(order = ['TimeStamp'])
            return ESD['ExitType'][-1] == 'Failure'
        except:
            return False
    else:
        return False
        

    
def LastTimeChanged(path):
    '''
    Returns last time, according to runtime meta data, that a  file (at "path")
    was actually modified (e.g. not simply overwritten, but actually modified.)
    '''


    actualmodtime = os.path.getmtime(path)
    if actualmodtime == FindPtime(path):
        try: 
            Data = tb.tabarray(SVfile = metapath,delimiter = ',', lineterminator='\n') 
            if len(Data) > 0:
                Data.sort(order=['TimeStamp'])
                Diffs = Data['Diff'].nonzero()[0]
                if len(Diffs) > 0:
                    return Data['TimeStamp'][Diffs[-1]]
                else:
                    return actualmodtime
            else:
                return actualmodtime
        except:
            return actualmodtime
    else:
        return actualmodtime
                                    
                                    
                                    
def FindPtime(target,Simple=False):
    '''
    Returns last time, according to runtime meta data, that 
    a target was succesfully created, if it is created data. 
    '''

    metapath = metadatapath(target) + '/CreationRecord.csv'
    if PathExists(metapath):
        try: 
            Data = tb.tabarray(SVfile = metapath,delimiter = ',', lineterminator='\n') 
            if len(Data) > 0:
                Data.sort(order=['TimeStamp'])
                if any(Data['ExitType'] == 'Success'):
                    MostRecentSuccess = Data[Data['ExitType'] == 'Success']['TimeStamp'][-1]
                    MoreRecentFailures = Data[(Data['ExitType'] == 'Failure') & (Data['TimeStamp'] > MostRecentSuccess)]
                    if len(MoreRecentFailures) > 0:
                        LeastRecentFailure = MoreRecentFailures['TimeStamp'][0]
                    else:
                        LeastRecentFailure = numpy.inf
                    return Data[(Data['TimeStamp'] >= MostRecentSuccess) & (Data['TimeStamp'] < LeastRecentFailure)]['TimeStamp'][-1] 
                else:
                    return numpy.nan
            else:
                return numpy.nan
        except:
            return numpy.nan
        else: pass
    else:
        return numpy.nan            


def metadatapath(datapath):
    return os.path.join(WORKING_DE.metadata_dir, datapath.strip('../'))
    
def opmetadatapath(oppath):
    return os.path.join(WORKING_DE.metadata_dir, oppath.replace('.','/'))
